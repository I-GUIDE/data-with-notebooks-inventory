{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e47f4e33-5c48-4422-99fc-ccae884b6d31",
   "metadata": {
    "tags": []
   },
   "source": [
    "# City-level Analysis of Human Sentiments of Heat Exposure Using Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132ed936-f233-4154-86a6-2b7dd9861196",
   "metadata": {},
   "source": [
    "Author: Fangzheng Lyu\n",
    "\n",
    "This notebook is related to the paper [Mapping dynamic human sentiments of heat exposure with location-based social media data](https://www.tandfonline.com/doi/full/10.1080/13658816.2024.2343063)\n",
    "\n",
    "One city-scale analysis was conducted using collected data in the City of Chicago. Unexpected hot weather was detected in Chicago with the highest temperature being 88 degrees Fahrenheit. In this case study, both census tracts and 1â€‰km spatial resolution, with approximately 800 spatial units each for the Cook County, are selected to ensure comprehensive area representation with the amount of social media getting collected. The two spatial unit we select are:\n",
    "- Census Tracts\n",
    "- 1km Spatial Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e096aa-1394-47a8-a8f8-e20b53559baf",
   "metadata": {},
   "source": [
    "## Notebook Outline\n",
    "- [Processing Twitter/X Data](#processing)\n",
    "- [Understanding How Human Sentiments of Heat Exposure from Tweet Posts](#understand)\n",
    "- [Aggregate the result to the census tract](#aggregate)\n",
    "- [Visualization - Census Tract](#census)\n",
    "- [Visualization - 1km Spatial Resolution](#1km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4acfeff-e276-4f81-9444-769368d9808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Library\n",
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import json\n",
    "from shapely.geometry import Polygon, Point, MultiPolygon\n",
    "import shapefile\n",
    "import re\n",
    "import shapefile as shp  # Requires the pyshp package\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861d3878-e98a-4f12-882e-17f0b0f64765",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='processing'></a>\n",
    "\n",
    "## 1. Processing Twitter/X Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fba3dc-088b-4609-9571-7311f376c239",
   "metadata": {},
   "source": [
    "The following cell will allow users to extract and filter the social media data Twitter/X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5903c555-d0ad-49ff-b5aa-dcf715fff131",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load chicago shapefile\n",
    "shapefile = gpd.read_file(\"./geo/geo_export_5bb8636f-65b7-450a-8fd9-7f01027fd84b.shp\")\n",
    "chicago_shape = shapefile[\"geometry\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b93e169-2624-4c2a-8214-9584208e8926",
   "metadata": {},
   "source": [
    "Filter all the Twitter/X data by location, find all data within the city of Chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9931ad1e-220b-48d3-a817-b68aaa215392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/250000-tweets-2021-09-25_04-59-49.json\n",
      "./data/250000-tweets-2021-09-26_01-46-49.json\n"
     ]
    }
   ],
   "source": [
    "## get the twitter in chicago\n",
    "## City scale analysis\n",
    "## This block of code will takes a long time\n",
    "## We iterate through all the twitter collected for find twitter in chicago\n",
    "## Get the filename\n",
    "filelist = os.listdir('./data/')\n",
    "filelist\n",
    "\n",
    "twitter_in_chicago = []\n",
    "\n",
    "# Opening JSON file\n",
    "for filename in filelist:\n",
    "    filepath = \"./data/\"+filename\n",
    "    print(filepath)\n",
    "    f = open(filepath)\n",
    "    data = json.load(f)\n",
    "    \n",
    "    ## Read the data if the centroid of the twitter point polygon lies within the boundary of the city of Chicago\n",
    "    for i in range(0, len(data)):\n",
    "        try:\n",
    "            ##Need to deal with case when the shapefile is too big\n",
    "            text = data[i][\"text\"]\n",
    "            t = data[i]['created_at']\n",
    "            ## Case 1\n",
    "            ## Twitter with exact geospatial location\n",
    "            if (data[i]['geo']!=None):\n",
    "                lat = data[i]['geo']['coordinates'][0]\n",
    "                lon = data[i]['geo']['coordinates'][1]\n",
    "                exact_loc = Point(lon, lat)\n",
    "                if chicago_shape.contains(exact_loc):\n",
    "                    ## print(\"inside\")\n",
    "                    twitter_in_chicago.append((exact_loc, t, text))\n",
    "            else:\n",
    "                ## Twitter with a polygon bounding box\n",
    "                poly = data[i]['place']['bounding_box'][\"coordinates\"][0]\n",
    "                lon = -1000\n",
    "                lat = -1000\n",
    "\n",
    "                lon = [p[0] for p in poly]\n",
    "                lat = [p[1] for p in poly]\n",
    "                centroid = (sum(lon) / len(poly), sum(lat) / len(poly))\n",
    "                point = Point(centroid)\n",
    "                ## check if a centroid is in the bounding box of chicago\n",
    "                if chicago_shape.contains(point):\n",
    "                    ## print(\"inside\")\n",
    "                    twitter_in_chicago.append((poly, t, text))\n",
    "        except:\n",
    "            ## no geographical location\n",
    "            pass\n",
    "    # Closing file\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490834e3-2bcf-47a3-a883-cfc0bfa53edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"There are in total \"+str(len(twitter_in_chicago))+\" geo-tagged Twitter Collected in Chicago in 9/25/2021 & 9/26/2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5b8f3b-aa58-4933-8317-662b91a78d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example of Twitter Message\n",
    "twitter_in_chicago[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156859e8-7ba0-4f2a-986f-66f9c8d2f699",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='understand'></a>\n",
    "\n",
    "## 2. Understanding How Human Sentiments of Heat Exposure from Tweet Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c11b7f-c49f-4010-b7e2-47db97373488",
   "metadata": {},
   "source": [
    "The following cell will allow users to apply heat dictionary generated using pretrained NLP model to understand the Twitter post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cba125-aef3-4e32-8bfc-8f8765df1092",
   "metadata": {},
   "source": [
    "A keyword-based NLP mehtod is adopted to the generated the heat dictionary. And the heat dictionary is used to access whether each Tweet post is talking about weather and how much is it talking about hot/cold weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d030f-c961-47a1-88a0-5c745f40193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the word heat dictionary\n",
    "f = open('./geo/data20000.txt','r')\n",
    "content = f.read()\n",
    "f.close()\n",
    "dict_word = {}\n",
    "content_list = content.split(\",\")\n",
    "for i in range(0,len(content_list)):\n",
    "    try:\n",
    "        word = content_list[i].split(\":\")[0].split(\"'\")[1]\n",
    "        #print(content_list[i].split(\":\"))\n",
    "        val = float(content_list[i].split(\":\")[1])\n",
    "        dict_word[word] = val\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1872d47-6b0f-4e82-815a-7c5d0be3a685",
   "metadata": {},
   "source": [
    "Apply the heat dictionary onto all the Tweets found in the city of Chicago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3654dc18-cbe7-4a5f-9138-1d57ae91e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Iterate through all twitter data in chicago\n",
    "\n",
    "d_twitter = []\n",
    "for i in range(0, len(twitter_in_chicago)):\n",
    "    loc = twitter_in_chicago[i][0]\n",
    "    t = twitter_in_chicago[i][1]\n",
    "    text = twitter_in_chicago[i][2]\n",
    "    res = re.findall(r'\\w+', text.lower())\n",
    "    val = 0\n",
    "    for word in res:\n",
    "        if word in dict_word.keys():\n",
    "            val = val + dict_word[word]\n",
    "    ## remove weather-irrelevant twitter\n",
    "    ## if none of the word in the heat dictionary show up \n",
    "    if (val!=0):\n",
    "        d_twitter.append((loc, t, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb82714-d3c1-4f8b-8fde-1924b2df4cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are \"+str(len(d_twitter))+\" weather-related Twitter in Chicago\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e76c5-d521-4588-9b67-5bf49f129a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dic = {}\n",
    "m_dic['Jan'] = 1\n",
    "m_dic['Feb'] = 2\n",
    "m_dic['Mar'] = 3\n",
    "m_dic['Apr'] = 4\n",
    "m_dic['May'] = 5\n",
    "m_dic['Jun'] = 6\n",
    "m_dic['Jul'] = 7\n",
    "m_dic['Aug'] = 8\n",
    "m_dic['Sep'] = 9\n",
    "m_dic['Oct'] = 10\n",
    "m_dic['Nov'] = 11\n",
    "m_dic['Dec'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3b4d2-aa31-4850-a840-13d8b29b7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the time difference between the current time and the Twitter post time\n",
    "today = datetime(2021,9, 25, 0)\n",
    "weather_related_twitter = []\n",
    "sec = []\n",
    "for twitter in d_twitter:\n",
    "    loc = twitter[0]\n",
    "    t = twitter[1].split()\n",
    "    val = twitter[2]\n",
    "    month = m_dic[t[1]]\n",
    "    day = int(t[2])\n",
    "    year = int(t[5])\n",
    "    hour = int(t[3].split(\":\")[0])\n",
    "    minute = int(t[3].split(\":\")[1])\n",
    "    twitter_t = datetime(year, month, day, hour, minute)\n",
    "    diff_minute = abs(twitter_t - today).total_seconds() / 60.0\n",
    "    weather_related_twitter.append((loc, diff_minute, val))\n",
    "    sec.append(diff_minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb016e1-c85f-45dd-8751-0019197708e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the temporal distributed with weather-related Tweets posted across time\n",
    "\n",
    "plt.hist(sec, bins=100, alpha=0.5)\n",
    "plt.title('When the tweets are posted')\n",
    "plt.xlabel('Minute')\n",
    "plt.ylabel('count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293be04f-0e87-4ce8-b330-3e679a155c48",
   "metadata": {},
   "source": [
    "<a id='aggregate'></a>\n",
    "\n",
    "## 3. Aggregate the result to the census tract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bebbc5-273b-4fe5-91e2-c01bd453b3ef",
   "metadata": {},
   "source": [
    "The following cell will allow users to aggregate the human sentiments of heat exposure from each Tweets to the spaital domain in the city of Chicago."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac6eded-284c-4a29-817d-ab35b26d5e06",
   "metadata": {},
   "source": [
    "Aggregate the result into census tract. [Inverse Distance Weighting (IDW)](https://en.wikipedia.org/wiki/Inverse_distance_weighting) is used when a census tract value is missinng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdcdbe6-888a-4acc-81f7-1c670d97ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Integrate into census tract level\n",
    "chicago = gpd.read_file(\"./Census_tract/geo_export_dc0b9c70-c036-4bcc-a602-8e9b9d36ea9f.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf67a2f-1ce0-4bd6-9498-620515938c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e5db8d-f609-4b6a-9c96-2dac73fb7a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a random point from a polygon\n",
    "import random\n",
    "\n",
    "def generate_random(number, polygon):\n",
    "    minx, miny, maxx, maxy = polygon.bounds\n",
    "    pnt = Point(random.uniform(minx, maxx), random.uniform(miny, maxy))\n",
    "    return pnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011174f-8602-4cd1-8a1d-b0b502cef22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to generate random location twitter\n",
    "## For monte caro experiment\n",
    "## Enable exact and poly if you want to see how many twitter has exact location and how many comes with a polygon\n",
    "#exact = 0\n",
    "#poly = 0\n",
    "def generate_random_loc(weather_related_twitter):\n",
    "    random_loc_twitter = []\n",
    "    for ele in weather_related_twitter:\n",
    "        loc = ele[0]\n",
    "        point = 0\n",
    "        #print(loc)\n",
    "        if (type(loc)==Point):\n",
    "            ## exact location extracted\n",
    "            point = loc\n",
    "            #exact = exact+1\n",
    "        else:\n",
    "            ## Select a random point from a multi-polygon\n",
    "            point = generate_random(1, Polygon(loc))\n",
    "            #poly = poly+1\n",
    "        random_loc_twitter.append([point, ele[2]])\n",
    "    return random_loc_twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be81b90d-8e01-486c-9b7c-72ff824a7ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_final_census_track = {}\n",
    "\n",
    "for i in range(0, 10):\n",
    "    ## try 10 random time\n",
    "    print(\"current \"+str(i))\n",
    "\n",
    "    ## Conduct kernel density estimation\n",
    "    random_loc_twitter = generate_random_loc(weather_related_twitter)\n",
    "\n",
    "    ### fill with inverse distance weighting\n",
    "\n",
    "\n",
    "    for index, row in chicago.iterrows():\n",
    "        key = index\n",
    "        ele = row['geometry'] \n",
    "        lon = ele.centroid.x\n",
    "        lat = ele.centroid.y\n",
    "        ## iterate through all the values in the existing twitter\n",
    "        up = 0\n",
    "        down = 0\n",
    "        IDW = 0\n",
    "        for twitter in random_loc_twitter:\n",
    "            pt = twitter[0]\n",
    "            curr_x = pt.x\n",
    "            curr_y = pt.y\n",
    "            curr_val = twitter[1]\n",
    "\n",
    "            distx = (curr_x-lon)*82\n",
    "            disty = (curr_y-lat)*111\n",
    "\n",
    "            w = 1/np.sqrt(distx*distx+disty*disty)\n",
    "\n",
    "            down = down+w\n",
    "            up = up+w*curr_val\n",
    "        rt = up/down\n",
    "\n",
    "        if (key not in d_final_census_track.keys()):\n",
    "            d_final_census_track[index]=[rt]\n",
    "        else:\n",
    "            d_final_census_track[index].append(rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13f7dc2-da3b-4744-b9ee-b573c4370fc3",
   "metadata": {},
   "source": [
    "Calculate Normalized Human Sentiments of Heat Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0868fb81-5cc4-4c6b-b251-7ae946757819",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_exposure_map_census_track = {}\n",
    "for key in d_final_census_track.keys():\n",
    "    ## Get the average hot exposure\n",
    "    heat_exposure_map_census_track[key] = np.mean(d_final_census_track[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda581f9-8356-406b-b2bd-06ad9c66af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nomalization to 0-1\n",
    "mn = min(heat_exposure_map_census_track.values())\n",
    "mx = max(heat_exposure_map_census_track.values())\n",
    "for key in heat_exposure_map_census_track.keys():\n",
    "    norm = (heat_exposure_map_census_track[key]-mn)/(mx-mn)\n",
    "    heat_exposure_map_census_track[key] = norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a168d90-c2dc-4cbe-96c0-c916bec614d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago[\"he_val\"]=list(heat_exposure_map_census_track.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ee5281-d544-41c1-ac61-00054759f005",
   "metadata": {},
   "source": [
    "Show the result geopandas dataframe for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40dd206-c213-4fe2-aa04-40307b679072",
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89315cf5-33ff-49e5-9440-b7fa734b8e2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='census'></a>\n",
    "\n",
    "## 4. Visualization - Census Tract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b829958-f5ac-48a7-845f-ecbf7b44aa23",
   "metadata": {},
   "source": [
    "The following cell conduct a census tract level analysis of human sentiments of urban heat in the city of Chicago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79ef9d9-14cd-4f58-a099-4d939ae066b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at how the heat exposure variable is distributed with a histogram\n",
    "chicago[\"he_val\"].hist(bins=40)\n",
    "plt.xlabel(\"Normalized Heat Exposure\")\n",
    "plt.ylabel(\"Number of census tracts\")\n",
    "plt.title(\"Human Centiments of Heat Exposure\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b3e672-7e0f-4ff3-bb15-1172e441f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Choropleth Map with geopandas \n",
    "chicago.plot(column = 'he_val', #Assign numerical data column\n",
    "                      legend = True, #Decide to show legend or not\n",
    "                      figsize = [20,10],\n",
    "                      cmap = 'YlOrRd',\n",
    "                      legend_kwds = {'label': \"Normalized Heat Exposure\"}) #Name the legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f5f73-7739-4921-ba73-7eb77b47e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "he_val= list(chicago['he_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b731a41-2f14-4a01-94c2-c1fa9847926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show the percentile value for 5 classes\n",
    "\n",
    "print(\"The 20th percentile is \" + str(np.percentile(he_val, 20)))\n",
    "print(\"The 40th percentile is \" + str(np.percentile(he_val, 40)))\n",
    "print(\"The 60th percentile is \" + str(np.percentile(he_val, 60)))\n",
    "print(\"The 80th percentile is \" + str(np.percentile(he_val, 80)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74303cd7-26aa-4535-ade0-bd548192f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Quantile Map\n",
    "chicago.plot(column = 'he_val', #Assign numerical data column\n",
    "                      scheme=\"quantiles\", \n",
    "                      k=5,\n",
    "                      legend = True, #Decide to show legend or not\n",
    "                      figsize = [20,10],\n",
    "                      cmap = 'YlOrRd') #Name the legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8118307-5c99-4d74-9afb-9ddb53391dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "he_val= list(chicago['he_val'])\n",
    "mn, mx = min(he_val), max(he_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc642932-4224-4410-be6c-ccec5188b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mn,mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189eb6f2-afa7-47ba-93cb-438285e457b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show the equal interval value for 5 class\n",
    "\n",
    "print(\"The 1th value is \" + str(1*(mx-mn)/5))\n",
    "print(\"The 2nd value is \" + str(2*(mx-mn)/5))\n",
    "print(\"The 3rd value is \" + str(3*(mx-mn)/5))\n",
    "print(\"The 4th value is \" + str(4*(mx-mn)/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a393a5-3824-44ca-9bc4-8b45b20ce744",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Equal Interval\n",
    "chicago.plot(column = 'he_val', #Assign numerical data column\n",
    "                      scheme=\"equal_interval\", \n",
    "                      k=5,\n",
    "                      legend = True, #Decide to show legend or not\n",
    "                      figsize = [20,10],\n",
    "                      cmap = 'YlOrRd') #Name the legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e5fc9-d1d8-4ae5-8143-f260138e9b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jenkspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8cfd59-f372-4944-8e48-07a7b4aabedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jenkspy\n",
    "he_val= list(chicago['he_val'])\n",
    "\n",
    "[a0, a1, a2, a3, a4, a5] = jenkspy.jenks_breaks(he_val, n_classes=5)\n",
    "print(\"The 1th value is \" + str(a1))\n",
    "print(\"The 2nd value is \" + str(a2))\n",
    "print(\"The 3rd value is \" + str(a3))\n",
    "print(\"The 4th value is \" + str(a4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac7faea-d8d5-4414-9d81-52e49d81c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Natural Break\n",
    "chicago.plot(column = 'he_val', #Assign numerical data column\n",
    "                      scheme=\"natural_breaks\", \n",
    "                      k=5,\n",
    "                      legend = True, #Decide to show legend or not\n",
    "                      figsize = [20,10],\n",
    "                      cmap = 'YlOrRd') #Name the legend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fefc23-d016-4750-8562-5d387d83bb65",
   "metadata": {},
   "source": [
    "<a id='1km'></a>\n",
    "\n",
    "## 5. Visualization - 1km Spatial Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae9c80-6a86-49bd-ad0a-af0796fcb851",
   "metadata": {},
   "source": [
    "The following cell conduct a analysis of human sentiments of urban heat at 1km spatial resolution in the city of Chicago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf09527-1e37-4d89-829f-f12e0e721380",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate raster based about 1 km spatial resoltuion \n",
    "## one Degree latitude = 111 km\n",
    "## In chicago, where latitude = 41.881832, one Degree longitude = 82 km\n",
    "## We use this estimation for the following ananlsis\n",
    "## This work as the city of Chicago is small\n",
    "lat_start = 41.05\n",
    "lon_start = -87.96\n",
    "\n",
    "incre_lat = 1/111\n",
    "incre_lon = 1/82\n",
    "\n",
    "lat_end = 42.05\n",
    "lon_end = -87.5\n",
    "\n",
    "raster = []\n",
    "\n",
    "lat = lat_start\n",
    "\n",
    "while(lat<lat_end):\n",
    "    lon = lon_start\n",
    "    while(lon<lon_end):\n",
    "        curr_point = Point(lon, lat)\n",
    "        if (curr_point.within(chicago_shape)):\n",
    "            raster.append([lon, lat])\n",
    "        lon = lon+incre_lon\n",
    "    lat = lat+incre_lat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466bcdf5-9b11-4c00-a9cd-e78220421de2",
   "metadata": {},
   "source": [
    "Generate reuslt for the human sentiments of heat exposure at different timeframe at fine temporal granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51627e68-6f73-4c69-9285-fca8c8a95602",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a random point from a polygon\n",
    "import random\n",
    "\n",
    "def generate_random(number, polygon):\n",
    "    minx, miny, maxx, maxy = polygon.bounds\n",
    "    pnt = Point(random.uniform(minx, maxx), random.uniform(miny, maxy))\n",
    "    return pnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2268e004-4556-423d-a4f6-0c2da1d4c354",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to generate random location twitter\n",
    "## For monte caro experiment\n",
    "## Enable exact and poly if you want to see how many twitter has exact location and how many comes with a polygon\n",
    "#exact = 0\n",
    "#poly = 0\n",
    "def generate_random_loc(weather_related_twitter):\n",
    "    random_loc_twitter = []\n",
    "    for ele in weather_related_twitter:\n",
    "        loc = ele[0]\n",
    "        point = 0\n",
    "        #print(loc)\n",
    "        if (type(loc)==Point):\n",
    "            ## exact location extracted\n",
    "            point = loc\n",
    "            #exact = exact+1\n",
    "        else:\n",
    "            ## Select a random point from a multi-polygon\n",
    "            point = generate_random(1, Polygon(loc))\n",
    "            #poly = poly+1\n",
    "        random_loc_twitter.append([point, ele[2]])\n",
    "    return random_loc_twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac43e12-a383-4d78-a902-35bccda83de6",
   "metadata": {},
   "source": [
    "Calcualte the human sentiments of heat exposure. Using Inverse Distance Weighting (IDW) for those spatial unit that doesn't have a points. And using Monte-Carlo simulation to take care of those multi-polygon locations in the social media posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87778752-e867-4f98-ae40-8ad2238f567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set seed for reproducibility\n",
    "## This may take a while\n",
    "\n",
    "d_final = {}\n",
    "\n",
    "for i in range(0, 10):\n",
    "    ## try 100 random time\n",
    "    print(\"current \"+str(i))\n",
    "\n",
    "    ## Conduct kernel density estimation\n",
    "    random_loc_twitter = generate_random_loc(weather_related_twitter)\n",
    "\n",
    "    ### fill with inverse distance weighting\n",
    "\n",
    "\n",
    "    for ele in raster:\n",
    "        lon = ele[0]\n",
    "        lat = ele[1]\n",
    "        ## iterate through all the values in the existing twitter\n",
    "        up = 0\n",
    "        down = 0\n",
    "        IDW = 0\n",
    "        for twitter in random_loc_twitter:\n",
    "            pt = twitter[0]\n",
    "            curr_x = pt.x\n",
    "            curr_y = pt.y\n",
    "            curr_val = twitter[1]\n",
    "\n",
    "            distx = (curr_x-lon)*82\n",
    "            disty = (curr_y-lat)*111\n",
    "\n",
    "            w = 1/np.sqrt(distx*distx+disty*disty)\n",
    "\n",
    "            down = down+w\n",
    "            up = up+w*curr_val\n",
    "        rt = up/down\n",
    "        \n",
    "        key = (ele[0],ele[1])\n",
    "        if (key not in d_final.keys()):\n",
    "            d_final[key]=[rt]\n",
    "        else:\n",
    "            d_final[key].append(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88572421-d6a4-4d72-8607-253a2428ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_exposure_map = {}\n",
    "for key in d_final.keys():\n",
    "    ## Get the average hot exposure\n",
    "    heat_exposure_map[key] = np.mean(d_final[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5232962f-9b19-45fe-b3b6-8abb3e92f268",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nomalization to 0-1\n",
    "mn = min(heat_exposure_map.values())\n",
    "mx = max(heat_exposure_map.values())\n",
    "for key in heat_exposure_map.keys():\n",
    "    norm = (heat_exposure_map[key]-mn)/(mx-mn)\n",
    "    heat_exposure_map[key] = norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c816183-c297-4330-96c8-50eeaedc6fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lonl = []\n",
    "latl = []\n",
    "var = []\n",
    "\n",
    "final_heat_exposure_map = {}\n",
    "for key in heat_exposure_map.keys():\n",
    "    \n",
    "    lonl.append(key[0])\n",
    "    latl.append(key[1])\n",
    "    var.append(heat_exposure_map[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7857dc-0ad0-41d3-b7e6-ab6773524ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.column_stack([lonl, latl, var]), \n",
    "                  columns=['lon', 'lat', 'val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11664002-7008-4010-a6c8-a33a7bc21c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99074d7-65a6-46cb-9c18-fcdccb80416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(title='Human Sentiments of Heat Expososure from 6 to 9', x='lon', y='lat', c='val', figsize = [10,10], subplots=True, marker=\"s\", s = 155, colormap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9f054-053d-43e7-adf9-5b029e99ee2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
