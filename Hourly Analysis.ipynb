{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c4a9778-d49e-4db9-aade-2d272c10ef5b",
   "metadata": {},
   "source": [
    "# Fine Granularity Analysis - Fine Granularity Human Sentiments of Heat Exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468e384c-519f-4f45-9c9c-c5ec4b494bbd",
   "metadata": {},
   "source": [
    "Author: Fangzheng Lyu\n",
    "\n",
    "This notebook is related to the paper [Mapping dynamic human sentiments of heat exposure with location-based social media data](https://www.tandfonline.com/doi/full/10.1080/13658816.2024.2343063)\n",
    "\n",
    "Evaulting Twitter-based heat exposure dynamics can be analyzed at a finer temporal resolution. In this study, we further looked at how the heat exposure changes on September 26th, 2021. On September 26rh, there is a significant temperate rises in Chicago where the highest temperature increased from 70 degrees Fahrenheit to 84 degrees Fahrenheit from September 25th, 2021, to September 26th, 2021. We investigated the dynamics of heat exposure at a fine temporal resolution to gain insight into the changes in peopleâ€™s exposure to dramatic temperature rises within a day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a85cca6-2329-4a0c-ab6f-e128f0706a48",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notebook Outline\n",
    "- [Read Twitter Data and Data Processing](#read)\n",
    "- [Understand the Human Sentiments of Heat Exposure of Twitter Data](#understand)\n",
    "- [Distribute All the Social Media Data into the 3-hour Temporal Bins](#Distribute)\n",
    "- [Visualization](#Visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e296ca-e514-440e-80d2-40a4a2be8716",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbc66dc7-886e-4fe0-8685-4e397ef67aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import json\n",
    "from shapely.geometry import Polygon, Point, MultiPolygon\n",
    "import shapefile\n",
    "import re\n",
    "import shapefile as shp  # Requires the pyshp package\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c96c81-f0a6-4b4b-80bf-1e26eccd8c99",
   "metadata": {},
   "source": [
    "<a id='read'></a>\n",
    "\n",
    "## 1. Read Twitter Data and data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc9d54d-5fe0-470a-9e02-4f24db0f6154",
   "metadata": {},
   "source": [
    "Read the shapefile of Chicago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92008218-5d97-4e7b-ac68-b286cd0c8232",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile = gpd.read_file(\"./geo/geo_export_5bb8636f-65b7-450a-8fd9-7f01027fd84b.shp\")\n",
    "chicago_shape = shapefile[\"geometry\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4a05ad-ccff-41e8-96df-ee287e2c3430",
   "metadata": {},
   "source": [
    "Retrieve the information of all the Twitter file available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d555ca6-448f-4a2b-b80e-40782a137d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['250000-tweets-2021-09-25_04-59-49.json',\n",
       " '250000-tweets-2021-09-26_01-46-49.json',\n",
       " '250000-tweets-2021-09-25_20-12-57.json',\n",
       " '250000-tweets-2021-09-26_19-43-23.json',\n",
       " '250000-tweets-2021-09-26_22-13-26.json',\n",
       " '250000-tweets-2021-09-25_17-48-51.json',\n",
       " '250000-tweets-2021-09-26_09-39-36.json',\n",
       " '250000-tweets-2021-09-26_07-36-30.json',\n",
       " '250000-tweets-2021-09-25_22-39-35.json',\n",
       " '250000-tweets-2021-09-25_13-24-18.json',\n",
       " '250000-tweets-2021-09-25_01-48-23.json',\n",
       " '250000-tweets-2021-09-26_11-29-11.json',\n",
       " '250000-tweets-2021-09-26_13-18-44.json',\n",
       " '250000-tweets-2021-09-25_09-29-48.json',\n",
       " '250000-tweets-2021-09-26_17-16-06.json',\n",
       " '250000-tweets-2021-09-26_15-10-52.json',\n",
       " '250000-tweets-2021-09-25_11-25-23.json',\n",
       " '250000-tweets-2021-09-25_07-33-30.json',\n",
       " '250000-tweets-2021-09-25_15-34-24.json',\n",
       " '250000-tweets-2021-09-26_04-56-08.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aef5e49-625f-410a-be67-a70f291a3c94",
   "metadata": {},
   "source": [
    "Find all the geo-tagged Twitter data in the city of Chicago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c72968-e103-4ebf-9c0d-1530ffd81459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/250000-tweets-2021-09-25_04-59-49.json\n",
      "./data/250000-tweets-2021-09-26_01-46-49.json\n"
     ]
    }
   ],
   "source": [
    "## get the twitter in chicago\n",
    "## City scale analysis\n",
    "## This block of code will takes a long time\n",
    "## We iterate through all the twitter collected for find twitter in chicago\n",
    "## Get the filename\n",
    "filelist = os.listdir('./data/')\n",
    "filelist\n",
    "\n",
    "twitter_in_chicago = []\n",
    "\n",
    "# Opening JSON file\n",
    "for filename in filelist:\n",
    "    filepath = \"./data/\"+filename\n",
    "    print(filepath)\n",
    "    f = open(filepath)\n",
    "    data = json.load(f)\n",
    "    \n",
    "    ## Read the data if the centroid of the twitter point polygon lies within the boundary of the city of Chicago\n",
    "    for i in range(0, len(data)):\n",
    "        try:\n",
    "            ##Need to deal with case when the shapefile is too big\n",
    "            text = data[i][\"text\"]\n",
    "            t = data[i]['created_at']\n",
    "            ## Case 1\n",
    "            ## Twitter with exact geospatial location\n",
    "            if (data[i]['geo']!=None):\n",
    "                lat = data[i]['geo']['coordinates'][0]\n",
    "                lon = data[i]['geo']['coordinates'][1]\n",
    "                exact_loc = Point(lon, lat)\n",
    "                if chicago_shape.contains(exact_loc):\n",
    "                    ## print(\"inside\")\n",
    "                    twitter_in_chicago.append((exact_loc, t, text))\n",
    "            else:\n",
    "                ## Twitter with a polygon bounding box\n",
    "                poly = data[i]['place']['bounding_box'][\"coordinates\"][0]\n",
    "                lon = -1000\n",
    "                lat = -1000\n",
    "\n",
    "                lon = [p[0] for p in poly]\n",
    "                lat = [p[1] for p in poly]\n",
    "                centroid = (sum(lon) / len(poly), sum(lat) / len(poly))\n",
    "                point = Point(centroid)\n",
    "                ## check if a centroid is in the bounding box of chicago\n",
    "                if chicago_shape.contains(point):\n",
    "                    ## print(\"inside\")\n",
    "                    twitter_in_chicago.append((poly, t, text))\n",
    "        except:\n",
    "            ## no geographical location\n",
    "            pass\n",
    "    # Closing file\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008af25d-84ba-4195-8b03-93b8a29b4624",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"There are in total \"+str(len(twitter_in_chicago))+\" geo-tagged Twitter Collected in Chicago\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c7a96b-ac52-4e6a-867e-551c0019f434",
   "metadata": {},
   "source": [
    "<a id='understand'></a>\n",
    "\n",
    "## 2. Understand the human sentiments of heat exposure of Twitter data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058f2b3-5191-4e53-8f5c-48b5677bc029",
   "metadata": {},
   "source": [
    "Read the heat dictionary of social media data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4047ef7-9082-4255-a166-089eee05ebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the word heat dictionary\n",
    "f = open('./geo/data20000.txt','r')\n",
    "content = f.read()\n",
    "f.close()\n",
    "dict_word = {}\n",
    "content_list = content.split(\",\")\n",
    "for i in range(0,len(content_list)):\n",
    "    try:\n",
    "        word = content_list[i].split(\":\")[0].split(\"'\")[1]\n",
    "        #print(content_list[i].split(\":\"))\n",
    "        val = float(content_list[i].split(\":\")[1])\n",
    "        dict_word[word] = val\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a82a4-4103-4355-8907-82893a35e9ae",
   "metadata": {},
   "source": [
    "Find all the weather-realted Twitter data in the city of Chicago from all the Twitter data in Chicago using the heat dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daefad10-4cc0-4509-9a14-9a17ca018c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Iterate through all twitter data in chicago\n",
    "\n",
    "d_twitter = []\n",
    "for i in range(0, len(twitter_in_chicago)):\n",
    "    loc = twitter_in_chicago[i][0]\n",
    "    t = twitter_in_chicago[i][1]\n",
    "    text = twitter_in_chicago[i][2]\n",
    "    res = re.findall(r'\\w+', text.lower())\n",
    "    val = 0\n",
    "    for word in res:\n",
    "        if word in dict_word.keys():\n",
    "            val = val + dict_word[word]\n",
    "    ## remove weather-irrelevant twitter\n",
    "    ## if none of the word in the heat dictionary show up \n",
    "    if (val!=0):\n",
    "        d_twitter.append((loc, t, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279cb4c4-2f81-4aa6-9bb6-4e172fde1c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are \"+str(len(d_twitter))+\" weather-related Twitter in Chicago\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43cab67-279f-4aef-ac05-49806e2b4bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dic = {}\n",
    "m_dic['Jan'] = 1\n",
    "m_dic['Feb'] = 2\n",
    "m_dic['Mar'] = 3\n",
    "m_dic['Apr'] = 4\n",
    "m_dic['May'] = 5\n",
    "m_dic['Jun'] = 6\n",
    "m_dic['Jul'] = 7\n",
    "m_dic['Aug'] = 8\n",
    "m_dic['Sep'] = 9\n",
    "m_dic['Oct'] = 10\n",
    "m_dic['Nov'] = 11\n",
    "m_dic['Dec'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1771d0-fb3b-427c-8990-982f1190af33",
   "metadata": {},
   "source": [
    "Set up the current datetime, the Central Time in the city of Chicago in the Summer is GMT-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c66417-68f1-470a-9109-04f91a62c005",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr = datetime(2021,9, 26, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12250d32-81c3-466d-83ca-c83e867c7a08",
   "metadata": {},
   "source": [
    "Find the time difference between the current time and the time when the Twitter was posted to conduct fine graunlarity analysis of human sentiments of heat exposure. And find all the weather-related Twitter related data that is collected within the 24 hour timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f52e0-eeda-4147-91b0-da584c4ad3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the time difference between the current time and the Twitter post time\n",
    "weather_related_twitter = []\n",
    "for twitter in d_twitter:\n",
    "    loc = twitter[0]\n",
    "    t = twitter[1].split()\n",
    "    val = twitter[2]\n",
    "    month = m_dic[t[1]]\n",
    "    day = int(t[2])\n",
    "    year = int(t[5])\n",
    "    hour = int(t[3].split(\":\")[0])\n",
    "    minute = int(t[3].split(\":\")[1])\n",
    "    twitter_t = datetime(year, month, day, hour, minute)\n",
    "    diff_hour = (twitter_t - curr).total_seconds() / (60.0*60)\n",
    "    if (diff_hour<0 or diff_hour>24):\n",
    "        #print(diff_hour)\n",
    "        pass\n",
    "    else:\n",
    "        weather_related_twitter.append((loc, diff_hour, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3afca0e-83f8-445e-9bf1-c6c76c93c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weather_related_twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a418d474-33cc-4221-93f5-81b1ff091adb",
   "metadata": {},
   "source": [
    "Find the maximum, minimun when the Twitter data is collected and also the number of Twitter getting collected within the 24-hour timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260d732d-59ad-4610-93b1-c89408e4bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for ele in weather_related_twitter:\n",
    "    t.append(ele[1])\n",
    "min(t),max(t),len(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9cb837-4256-4ac1-a0b3-3c759d95f276",
   "metadata": {},
   "source": [
    "<a id='Distribute'></a>\n",
    "\n",
    "## 3. Distribute All the Social Media Data into the 3-hour Temporal Bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45649bf-8989-48af-9012-70cac8aba4a8",
   "metadata": {},
   "source": [
    "Put all the Twitter collected in the 24-hour timeframe into 3-hour temporal bins. There are 8 different seperate timeslice and each one is of 3 hours. The 8 bins correspond to the timeframe at 0-3, 3-6, 6-9. 9-12. 12-15, 15-18, 18-21, 21-24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41acd7aa-82e4-4780-b3d9-0c658f13babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = []\n",
    "s2 = []\n",
    "s3 = []\n",
    "s4 = []\n",
    "s5 = []\n",
    "s6 = []\n",
    "s7 = []\n",
    "s8 = []\n",
    "for ele in weather_related_twitter:\n",
    "    t = ele[1]\n",
    "    if (t>0 and t<=3):\n",
    "        s1.append(ele)\n",
    "    elif (t>3 and t<=6):\n",
    "        s2.append(ele)\n",
    "    elif (t>6 and t<=9):\n",
    "        s3.append(ele)\n",
    "    elif (t>9 and t<=12):\n",
    "        s4.append(ele)\n",
    "    elif (t>12 and t<=15):\n",
    "        s5.append(ele)\n",
    "    elif (t>15 and t<=18):\n",
    "        s6.append(ele)\n",
    "    elif (t>18 and t<=21):\n",
    "        s7.append(ele)\n",
    "    elif (t>21 and t<=24):\n",
    "        s8.append(ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51f9aa6-71e5-418f-85a1-e564cd7a9eec",
   "metadata": {},
   "source": [
    "Get how many weather-related Twitter posts are falling into each time bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79c545-1dd5-4c0a-adb5-5f06c4fdf52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s1),len(s2),len(s3),len(s4),len(s5),len(s6), len(s7),len(s8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21550686-abc1-46a5-bc75-213951b26eba",
   "metadata": {},
   "source": [
    "<a id='Visualization'></a>\n",
    "\n",
    "## 4. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4601320-5b23-4044-b295-84663d244eed",
   "metadata": {},
   "source": [
    "Visualize the fine-temporal-granularity maps for human sentiments of heat exposure.\n",
    "\n",
    "The analysis in condected at 1km * 1km spatial resoluiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395b26d-d81b-45ca-8c7a-b9502d71b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate raster based about 1 km spatial resoltuion \n",
    "## one Degree latitude = 111 km\n",
    "## In chicago, where latitude = 41.881832, one Degree longitude = 82 km\n",
    "## We use this estimation for the following ananlsis\n",
    "## This work as the city of Chicago is small\n",
    "lat_start = 41.05\n",
    "lon_start = -87.96\n",
    "\n",
    "incre_lat = 1/111\n",
    "incre_lon = 1/82\n",
    "\n",
    "lat_end = 42.05\n",
    "lon_end = -87.5\n",
    "\n",
    "raster = []\n",
    "\n",
    "lat = lat_start\n",
    "\n",
    "while(lat<lat_end):\n",
    "    lon = lon_start\n",
    "    while(lon<lon_end):\n",
    "        curr_point = Point(lon, lat)\n",
    "        if (curr_point.within(chicago_shape)):\n",
    "            raster.append([lon, lat])\n",
    "        lon = lon+incre_lon\n",
    "    lat = lat+incre_lat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb4882b-6c0a-49d5-b25d-6e65971507b4",
   "metadata": {},
   "source": [
    "Generate reuslt for the human sentiments of heat exposure at different timeframe at fine temporal granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac2ec1-ec38-4b13-b5f3-51850021cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a random point from a polygon\n",
    "import random\n",
    "\n",
    "def generate_random(number, polygon):\n",
    "    minx, miny, maxx, maxy = polygon.bounds\n",
    "    pnt = Point(random.uniform(minx, maxx), random.uniform(miny, maxy))\n",
    "    return pnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09248ea-9dea-4b17-b59b-e0a3305c5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to generate random location twitter\n",
    "## For monte caro experiment\n",
    "## Enable exact and poly if you want to see how many twitter has exact location and how many comes with a polygon\n",
    "#exact = 0\n",
    "#poly = 0\n",
    "def generate_random_loc(weather_related_twitter):\n",
    "    random_loc_twitter = []\n",
    "    for ele in weather_related_twitter:\n",
    "        loc = ele[0]\n",
    "        point = 0\n",
    "        #print(loc)\n",
    "        if (type(loc)==Point):\n",
    "            ## exact location extracted\n",
    "            point = loc\n",
    "            #exact = exact+1\n",
    "        else:\n",
    "            ## Select a random point from a multi-polygon\n",
    "            point = generate_random(1, Polygon(loc))\n",
    "            #poly = poly+1\n",
    "        random_loc_twitter.append([point, ele[2]])\n",
    "    return random_loc_twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541324f5-12d1-46f9-8d0c-10280dc8616e",
   "metadata": {},
   "source": [
    "Calcualte the human sentiments of heat exposure. Using Inverse Distance Weighting (IDW) for those spatial unit that doesn't have a points. And using Monte-Carlo simulation to take care of those multi-polygon locations in the social media posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5894c6e9-7c93-45fd-8d70-095b1d1247e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for 8 bins\n",
    "def get_heat_exposure(weather_related_twitter):\n",
    "    d_final = {}\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        ## try 10 random time\n",
    "        #print(\"current \"+str(i))\n",
    "\n",
    "        ## Conduct kernel density estimation\n",
    "        random_loc_twitter = generate_random_loc(weather_related_twitter)\n",
    "\n",
    "        ### fill with inverse distance weighting\n",
    "\n",
    "\n",
    "        for ele in raster:\n",
    "            lon = ele[0]\n",
    "            lat = ele[1]\n",
    "            ## iterate through all the values in the existing twitter\n",
    "            up = 0\n",
    "            down = 0\n",
    "            IDW = 0\n",
    "            for twitter in random_loc_twitter:\n",
    "                pt = twitter[0]\n",
    "                curr_x = pt.x\n",
    "                curr_y = pt.y\n",
    "                curr_val = twitter[1]\n",
    "\n",
    "                distx = (curr_x-lon)*82\n",
    "                disty = (curr_y-lat)*111\n",
    "\n",
    "                w = 1/np.sqrt(distx*distx+disty*disty)\n",
    "\n",
    "                down = down+w\n",
    "                up = up+w*curr_val\n",
    "            rt = up/down\n",
    "\n",
    "            key = (ele[0],ele[1])\n",
    "            if (key not in d_final.keys()):\n",
    "                d_final[key]=[rt]\n",
    "            else:\n",
    "                d_final[key].append(rt)\n",
    "    return d_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d474e8b2-9b1c-4e11-8d33-67a1400597e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_rt = get_heat_exposure(s1)\n",
    "s2_rt = get_heat_exposure(s2)\n",
    "s3_rt = get_heat_exposure(s3)\n",
    "s4_rt = get_heat_exposure(s4)\n",
    "s5_rt = get_heat_exposure(s5)\n",
    "s6_rt = get_heat_exposure(s6)\n",
    "s7_rt = get_heat_exposure(s7)\n",
    "s8_rt = get_heat_exposure(s8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7da863-ab7e-4693-849b-415c3dcb0bf1",
   "metadata": {},
   "source": [
    "Get the heat exposure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d764464-e838-4385-ad8e-e32c2fcd89f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_exposure_map_1 = {}\n",
    "for key in s1_rt.keys():\n",
    "    ## Get the average hot exposure\n",
    "    heat_exposure_map_1[key] = np.mean(s1_rt[key])\n",
    "    \n",
    "heat_exposure_map_2 = {}\n",
    "for key in s2_rt.keys():\n",
    "    ## Get the average hot exposure\n",
    "    heat_exposure_map_2[key] = np.mean(s2_rt[key])\n",
    "    \n",
    "heat_exposure_map_3 = {}\n",
    "for key in s3_rt.keys():\n",
    "    ## Get the average hot exposure\n",
    "    heat_exposure_map_3[key] = np.mean(s3_rt[key])\n",
    "    \n",
    "heat_exposure_map_4 = {}\n",
    "for key in s4_rt.keys():\n",
    "    ## Get the average hot exposure\n",
    "    heat_exposure_map_4[key] = np.mean(s4_rt[key])\n",
    "    \n",
    "heat_exposure_map_5 = {}\n",
    "for key in s5_rt.keys():\n",
    "    ## Get the average hot exposure\n",
    "    heat_exposure_map_5[key] = np.mean(s5_rt[key])\n",
    "    \n",
    "heat_exposure_map_6 = {}\n",
    "for key in s6_rt.keys():\n",
    "    ## Get the average hot exposure\n",
    "    heat_exposure_map_6[key] = np.mean(s6_rt[key])\n",
    "    \n",
    "heat_exposure_map_7 = {}\n",
    "for key in s7_rt.keys():\n",
    "    ## Get the average hot exposure\n",
    "    heat_exposure_map_7[key] = np.mean(s7_rt[key])\n",
    "    \n",
    "heat_exposure_map_8 = {}\n",
    "for key in s8_rt.keys():\n",
    "    ## Get the average hot exposure\n",
    "    heat_exposure_map_8[key] = np.mean(s8_rt[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90219f87-5588-4f83-bf9c-e954cc62537c",
   "metadata": {},
   "source": [
    "Get the maximum and minimum to normalize the human sentiments of heat exposure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2caeaa2-48e3-4852-a67f-6d3afeea719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = min(min(heat_exposure_map_1.values()),min(heat_exposure_map_2.values()),min(heat_exposure_map_3.values()),min(heat_exposure_map_4.values()),min(heat_exposure_map_5.values()),min(heat_exposure_map_6.values()), min(heat_exposure_map_7.values()),min(heat_exposure_map_8.values()))\n",
    "mx = max(max(heat_exposure_map_1.values()),max(heat_exposure_map_2.values()),max(heat_exposure_map_3.values()),max(heat_exposure_map_4.values()),max(heat_exposure_map_5.values()),max(heat_exposure_map_6.values()), max(heat_exposure_map_7.values()),max(heat_exposure_map_8.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d1f7f7-3322-4bc3-a83c-69cda0b66438",
   "metadata": {},
   "outputs": [],
   "source": [
    "mn, mx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7468d6ce-7ac0-45bb-96f7-a916ca5a1a46",
   "metadata": {},
   "source": [
    "Calcualte the normalized human sentiments of heat exposure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b5ec2e-9748-422f-8881-b27734bb8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in heat_exposure_map_1.keys():\n",
    "    norm = (heat_exposure_map_1[key]-mn)/(mx-mn)\n",
    "    heat_exposure_map_1[key] = norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833713e-0db7-44ad-8bb9-44c9c1a9419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in heat_exposure_map_2.keys():\n",
    "    norm = (heat_exposure_map_2[key]-mn)/(mx-mn)\n",
    "    heat_exposure_map_2[key] = norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf3ad5-7d30-4311-a2fd-4360f9a79fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in heat_exposure_map_3.keys():\n",
    "    norm = (heat_exposure_map_3[key]-mn)/(mx-mn)\n",
    "    heat_exposure_map_3[key] = norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0623a14d-2d4e-4845-a997-e76e24217752",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in heat_exposure_map_4.keys():\n",
    "    norm = (heat_exposure_map_4[key]-mn)/(mx-mn)\n",
    "    heat_exposure_map_4[key] = norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062b7c68-32a9-433e-920d-c181122949d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in heat_exposure_map_5.keys():\n",
    "    norm = (heat_exposure_map_5[key]-mn)/(mx-mn)\n",
    "    heat_exposure_map_5[key] = norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d65cb7-db5c-4d30-969b-1bffa0368b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in heat_exposure_map_6.keys():\n",
    "    norm = (heat_exposure_map_6[key]-mn)/(mx-mn)\n",
    "    heat_exposure_map_6[key] = norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062c30f5-81c6-4188-83e2-54e856ce94b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in heat_exposure_map_7.keys():\n",
    "    norm = (heat_exposure_map_7[key]-mn)/(mx-mn)\n",
    "    heat_exposure_map_7[key] = norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf32de6a-07bc-4ee8-9430-59d1d9b57d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in heat_exposure_map_8.keys():\n",
    "    norm = (heat_exposure_map_8[key]-mn)/(mx-mn)\n",
    "    heat_exposure_map_8[key] = norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2ea741-e93d-4802-b617-4a1b938cf515",
   "metadata": {},
   "source": [
    "Create dataframe for visualzation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68afb522-eb6a-4760-b52b-a610f2999b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "lonl = []\n",
    "latl = []\n",
    "var1 = []\n",
    "var2 = []\n",
    "var3 = []\n",
    "var4 = []\n",
    "var5 = []\n",
    "var6 = []\n",
    "var7 = []\n",
    "var8 = []\n",
    "\n",
    "final_heat_exposure_map = {}\n",
    "for key in heat_exposure_map_1.keys():\n",
    "    \n",
    "    lonl.append(key[0])\n",
    "    latl.append(key[1])\n",
    "    var1.append(heat_exposure_map_1[key])\n",
    "    var2.append(heat_exposure_map_2[key])\n",
    "    var3.append(heat_exposure_map_3[key])\n",
    "    var4.append(heat_exposure_map_4[key])\n",
    "    var5.append(heat_exposure_map_5[key])\n",
    "    var6.append(heat_exposure_map_6[key])\n",
    "    var7.append(heat_exposure_map_7[key])\n",
    "    var8.append(heat_exposure_map_8[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe5f7f4-572a-4bfe-8b6e-b2942bf8380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.column_stack([lonl, latl, var1, var2, var3, var4, var5, var6, var7, var8]), \n",
    "                  columns=['lon', 'lat', '0-3', '3-6', '6-9', '9-12', '12-15', '15-18', '18-21', '21-24'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b5b07-0072-4581-a0d4-b74d470d7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb0266-0a74-4a83-b7a7-4d8c9a9d3b9d",
   "metadata": {},
   "source": [
    "Visualize result for 6-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c26d466-9b57-4866-9ef9-261a168cb578",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(title='Human Sentiments of Heat Expososure from 6 to 9', x='lon', y='lat', c='6-9', figsize = [10,10], subplots=True, marker=\"s\", s = 155, colormap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc1d7e-b8a7-4877-8a5e-dfc2b3db06b3",
   "metadata": {},
   "source": [
    "Visualize result for 9-12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e5df6-dfdf-4e88-995f-9b2b2a08c9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(title='Human Sentiments of Heat Expososure from 9 to 12', x='lon', y='lat', c='9-12', figsize = [10,10], subplots=True, marker=\"s\", s = 155, colormap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841eb8c3-bfe9-40ec-b787-7734da964d36",
   "metadata": {},
   "source": [
    "Visualize result for 12-15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ae672-b67e-4704-be88-95b50e1616ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(title='Human Sentiments of Heat Expososure from 12 to 15', x='lon', y='lat', c='12-15', figsize = [10,10], subplots=True, marker=\"s\", s = 155, colormap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45b111d-8290-4e8b-91b5-aebfaf75a744",
   "metadata": {},
   "source": [
    "Visualize result for 15-18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c486d-4416-4c1d-90fe-b978080733b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(title='Human Sentiments of Heat Expososure from 15 to 18', x='lon', y='lat', c='15-18', figsize = [10,10], subplots=True, marker=\"s\", s = 155, colormap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd51809f-0d7a-4521-99ab-6ccaf7f67a68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
